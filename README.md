# Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding

<p align="center">
        <a href="https://arxiv.org/abs/2403.18593">Paper</a>
</p>

<p align="center">
    <img src="assets/HOOK.webp" width="700"/>
<p>

This is the official pytorch implementation of HOOK.

_\* The above conceptual image was generated by DALL-E_

## Abstract
The paradigm shift introduced by multimodal large language models, which is based on the transformer architecture and the pretext task of "next-token prediction," has revolutionized the field of remote sensing image understanding. However, the tokenizer, as one of the fundamental components of large models, has long been overlooked or even misunderstood in visual tasks. One key factor of the great comprehension power of the large language model is that natural language tokenizers utilize meaningful words or subwords as the basic elements of language. In contrast, mainstream visual tokenizers, represented by patch-based methods such as Patch Embed, rely on meaningless rectangular patches as basic elements of vision, which cannot serve as effectively as words or subwords in language. Starting from the essence of the tokenizer, we defined semantically independent regions (SIRs) for vision. Based on this definition, we propose two properties that an ideal visual tokenizer should possess: (1) homogeneity, where SIRs serve as the basic elements of vision, and (2) adaptivity, which allows for a flexible number of tokens to accommodate images of any size and tasks of any granularity. Based on this, we designed a simple HOmogeneous visual tOKenizer: HOOK. HOOK mainly consists of two modules: the Object Perception Module (OPM) and the Object Vectorization Module (OVM). To achieve homogeneity, the OPM splits the image into 4×4 pixel seeds and then utilizes the attention mechanism to perceive SIRs. The OVM employs cross-attention to merge seeds within the same SIR. To achieve adaptability, the OVM defines a variable number of learnable vectors as cross-attention queries, allowing for the adjustment of token quantity. We conducted experiments on the NWPU-RESISC45, WHU-RS19 classification dataset, and GID5 segmentation dataset for sparse and dense tasks. The results demonstrate that the visual tokens obtained by HOOK correspond to individual objects, which demonstrates homogeneity. HOOK outperformed Patch Embed by 6% and 10% in the two tasks and achieved state-of-the-art performance compared to the baselines used for comparison. Compared to Patch Embed, which requires more than one hundred tokens for one image, HOOK requires only 6 and 8 tokens for sparse and dense tasks, respectively, resulting in efficiency improvements of 1.5 to 2.8 times. The homogeneity and adaptability we proposed provide new perspectives on the study of visual tokenizers, and guided by these properties, the HOOK we designed shows potential to replace Patch Embed.


Tokenizer作为大语言模型的基本组件之一，却在包括在遥感影像理解在内的是视觉任务中被长期忽视甚至误解：LLM模型具有强大的自然语言理解能力，其中一个重要原因是自然语言Tokenizer以有意义的word或者sub-word作为语言的基本元素；而目前主流visual tokenizer属于以Patch Embed为代表的Patch-based方法，无意义的规则矩形patch无法像word或sub-word那样作为图像的基本元素。我们提出语义独立区域（SIR）概念，认为理想的visual tokenizer 应该具备两个基本性质：(1) 均一性：语义独立区域作为图像的基本元素；(2) 适应性：token数量可变以适应任意大小的影像和任意粒度的任务。基于此，我们设计了一个简单的HOmogeneous visual tOKenizer：HOOK。HOOK主要包含两个模块：Object Perception Module（OPM）和Object Vectorization Module（OVM）。为了实现均一性，OPM将图像分割为若干4×4像素窗口的特征，随后由注意力机制来感知语义独立区域，最后OVM利用Cross-Attention合并同一语义独立区域内的特征；为了实现自适应性，OVM定义了N个可学习的向量作为Cross-Attention的Queries，其中N为可变的超参以达到任意调整token数量的目的。我们在NWPU-RESISC45、WHU-RS19分类数据集和GID5分割数据集上分别测试了HOOK在稀疏和密集任务上的性能，结果表明：HOOK得到的 visual token各自对应一类地物，具备了均一性的性质，两类任务上的精度也分别超越Patch Embed 6%和10%，且在我们对比的基线中达到了SOTA；相较于需要上百个token的Patch Embed，HOOK在稀疏和密集任务上分别仅需要6个和8个token，效率提升了1.5倍到2.8倍。我们提出的均一性和适应性为isual tokenizer的研究提供了新的视角，且在上述两个性质的指导下，我们设计的HOOK显示出取代Patch Embed的潜力。

## Performance
| Datasets | Visual tokenizer | Performance | Log |
| -------- | ---------------- | ----------- | --- |
| NWPU     | Patch Embed      | 70.30       | <a href="assets/logs/NWPU_PE.log">log</a> |
| NWPU     | HOOK             | **77.38**   | <a href="assets/logs/NWPU_HOOK.log">log</a> |
| WHURS19  | Patch Embed      | 78.10       | <a href="assets/logs/RS19_PE.log">log</a> |
| WHURS19  | HOOK             | **87.58**   | <a href="assets/logs/RS19_HOOK.log">log</a> |
| GID5     | Patch Embed      | 67.79       | <a href="assets/logs/GID_PE.log">log</a> |
| GID5     | HOOK             | **78.81**   | <a href="assets/logs/GID_HOOK.log">log</a> |

## Installation
### Requirements
- Python 3.8 and above
- pytorch 2.0 and above
- CUDA
### Installing dependencies
1. Create a Virtual Environment

```
conda create -n Hook python=3.8 -y
conda activate Hook
```

2. Install pytorch

```
conda install pytorch==2.0.1 torchvision==0.15.2 torchaudio==2.0.2 pytorch-cuda=11.7 -c pytorch -c nvidia
```

3. Install dependent libraries

```
pip install -r requirements.txt
```

## Datasets
1. download

The datasets we used are publicly available on [Baidu Cloud(code:ep6g)](https://pan.baidu.com/s/1UFkzAKLbs70Ztw_ZyFZvfA?pwd=ep6g) and [Google Cloud](https://drive.google.com/file/d/1N-vQ9Hu6f6PhSpcGN70hJdVJrS_Cbx7v/view?usp=sharing).

2. Prepare datasets

Modify part of the code in the following three files as shown in the example below, and change the path to your own path:

```python
# token_datasets/NWPU_RESISC45.py
@staticmethod
def get_cfg():
    return dict(
        root_path='/media/shaorun/zl/Dataset/HOOK_datasets/NWPU/NWPU-RESISC45',
        metainfo_train='/media/shaorun/zl/Dataset/HOOK_datasets/NWPU/train.csv',
        metainfo_test='/media/shaorun/zl/Dataset/HOOK_datasets/NWPU/test.csv'
    )
```

```python
# token_datasets/WHU_RS19.py
@staticmethod
def get_cfg():
    return dict(
        root_path='/media/shaorun/zl/Dataset/HOOK_datasets/RS19/WHU-RS19',
        metainfo_train='/media/shaorun/zl/Dataset/HOOK_datasets/RS19/train0.5.csv',
        metainfo_test='/media/shaorun/zl/Dataset/HOOK_datasets/RS19/test0.5.csv'
    )
```

```python
# token_datasets/GID.py
@staticmethod
def get_cfg():
    return dict(
        train_imgdir = "/media/shaorun/zl/Dataset/HOOK_datasets/GID/train",
        train_anndir = "/media/shaorun/zl/Dataset/HOOK_datasets/GID/train_labels",
        test_imgdir = "/media/shaorun/zl/Dataset/HOOK_datasets/GID/test",
        test_anndir = "/media/shaorun/zl/Dataset/HOOK_datasets/GID/test_labels",
    )
```

## Training
### HOOK
- Sparse task (Classification)

```
# NWPU_RESISC45
python main_classification.py --exp-name HOOK_NWPU_100e_vis --seed 0 --token-num 6 --vis --dataset NWPU_RESISC45

# WHU-RS19
python main_classification.py --exp-name HOOK_NWPU_100e_vis --seed 0 --token-num 6 --vis --dataset WHURS19
```

- Dense task (Segmentation)

```
# GID
python main_segmentation.py --exp-name HOOK_GID_100e_vis --seed 0 --token-num 8 --vis --dataset GID
```

The meaning of the argument `--vis` is to output visualization results of homogeneous visual tokens during evaluation, removing this parameter will not output.

### Patch Embed
- Sparse task (Classification)

```
# NWPU_RESISC45
python pe_classification.py --exp-name PE_NWPU_100e --seed 0 --dataset NWPU_RESISC45

# WHU-RS19
python pe_classification.py --exp-name PE_WHURS19_100e --seed 0 --dataset WHURS19
```

- Dense task (Segmentation)

```
python pe_segmentation.py --exp-name PE_GID_100e --seed 0 --dataset GID
```

More controllable arguments are shown in the file: "pe_classification.py", "pe_segmentation.py", "main_classification.py" and "main_segmentation.py".

## Citation
If you find our paper and code useful in your research, please consider giving a star and citation :)

```BibTeX
@misc{shao2024homogeneous,
      title={Homogeneous Tokenizer Matters: Homogeneous Visual Tokenizer for Remote Sensing Image Understanding}, 
      author={Run Shao and Zhaoyang Zhang and Chao Tao and Yunsheng Zhang and Chengli Peng and Haifeng Li},
      year={2024},
      eprint={2403.18593},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}
```
